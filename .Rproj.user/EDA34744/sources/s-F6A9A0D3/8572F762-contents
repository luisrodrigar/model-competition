---
title: "Programming in R Project"
author: "Adson Costanzi Filho"
date: "November 4^th^, 2021"
output: 
  html_document:
    css: styles.css
    code_folding: hide
    toc: true
    toc_float: true
    toc_depth: 4
    theme: flatly
---

```{r setup, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
require(dplyr)
require(ggplot2)
library(kableExtra)
require(reactable)
require(reactablefmtr)

source('../theme_ggplot/theme_report.R')
source('../theme_ggplot/scale_color_report.R')
source('../theme_ggplot/scale_fill_report.R')
```


## Introduction

The goal of this project is to demonstrate some concepts that we learned in the "Programming in R" course applied to real data. For that purpose we are going to use a data set from [kaggle](https://www.kaggle.com/stefanoleone992/fifa-21-complete-player-dataset?select=players_21.csv) of the FIFA 2021 game, which is a very popular soccer simulator game available for many different platforms like PlayStation, Xbox and PC. Of course the FIFA 2021 game does not contemplate all the soccer players from around the world, but it can be seeing as very fair sample of the real population of professional soccer players. We are going to explore this data set ether descriptively, testing some hypothesis like: "Is the Player preferred foot significant to the player's overall score?", and also testing some Machine Learning Algorithms using the R package `caret` to predict the if the player prefer to use the Left foot.

## About the data

This data set includes 18944 rows (one for each player) and 18 columns. The data refers to the year 2021, it has players from 162 countries and it contemplates the 53 most important soccer leagues. However, with respect to this project, we will only focus on the following 7 variables:

* **sofifa_id**: It is an unique number indicating the player's ID.

* **long_name**: It is a string indicating the player's complete name.

* **age** (quantitative, continuous): It is a numeric variable indicating the player's age in years.

* **weight_kg** (quantitative, continuous):  It is a numeric variable indicating the player's weight in kg.

* **preferred_foot** (qualitative, nominal): It is a categorical variable indicating the player's preferred foot. The possible values are "Left" or "Right".

* **nationality** (qualitative, nominal): It is a categorical variable indicating the player's nationality. The possible values are "Argentina",  "Portugal", "Brazil", etc.

* **overall** (quantitative, continuous): It is a numerical variable that indicates the player's overall score and it is used to measure the player's overall soccer skill. The better the player's skill, the better the overall score.

Below it is possible to see a sample of the data:

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}
require(dplyr)

# reading the data
data <- read.csv("../data/fifa21_players.csv") %>%
  select(
    sofifa_id,
    long_name,
    age,
    weight_kg,
    preferred_foot,
    nationality,
    overall
  )

# selecting the a sample size 5
set.seed(1992)
row_sample <- sample(x = nrow(data), size = 5, replace = FALSE)
data_sample <- data[row_sample,]

knitr::kable(data_sample)
  
```


### Basic Descriptives

In order to understand and summarize our target data set we are going to perform some descriptive analysis as follows.

#### **age**

The average player's age is `r round(mean(data$age),2)` years old, the median is equal to `r round(median(data$age),2)` years old, the oldest player is `r round(max(data$age),2)` years old, the youngest player is `r round(min(data$age),2)` years old, and the standard deviation is `r round(sd(data$age),2)` years old.

Below it is represented the histogram and the density of the variable age. It is possible to see that the distribution of age is a little asymmetric with a tail in the right.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
require(ggplot2)

source('../theme_ggplot/theme_report.R')
source('../theme_ggplot/scale_color_report.R')
source('../theme_ggplot/scale_fill_report.R')

age_avg <- round(mean(data$age),2)
age_median <- round(median(data$age),2)

# histogram and density
data %>%
  ggplot(aes(x = age)) +
  geom_histogram(
    aes(y = ..density..),
    bins = 12, 
    fill = "#509997", 
    color = "black") +
  xlim(c(15, 45)) +
  geom_density(color = "#761954", size = 1) +
  geom_vline(
    xintercept = age_avg, 
    linetype = "dashed", 
    color = "#c62d64",
    size = 1.5) +
   geom_text(aes(age_avg, 0, label = paste("Average:", age_avg), hjust = -0.2, vjust = -1), color = "#c62d64") +
  geom_vline(
    xintercept = age_median, 
    linetype = "dotted", 
    color = "#87ca9d",
    size = 1.5) +
   geom_text(aes(age_median, 0, label = paste("Median:", age_median), hjust = 1.2, vjust = -1), color = "#87ca9d") +
  ggtitle("Distribution of player's age") +
  theme_report() +
  ylab("Age (years)") +
  xlab("Density")

```

#### **weight_kg**

The average player's weight is `r round(mean(data$weight_kg),2)`kg, the median is equal to `r round(median(data$weight_kg),2)`kg, the maximum is equal to `r round(max(data$weight_kg),2)`kg, the minimum is equal to `r round(min(data$weight_kg),2)`kg, and the standard deviation is `r round(sd(data$weight_kg),2)`kg.

Below it is represented the histogram and the density of the variable age. It is possible to see that the distribution of weight is pretty much symmetric around the average (`r round(mean(data$weight_kg),2)`kg).

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}

weight_avg <- round(mean(data$weight_kg),2)
weight_median <- round(median(data$weight_kg),2)

# histogram and density
data %>%
  ggplot(aes(x = weight_kg)) +
  geom_histogram(
    aes(y = ..density..),
    bins = 15, 
    fill = "#509997", 
    color = "black") +
  xlim(c(50, 110)) +
  geom_density(color = "#761954", size = 1) +
  geom_vline(
    xintercept = weight_avg, 
    linetype = "dashed", 
    color = "#c62d64",
    size = 1.5) +
   geom_text(aes(weight_avg, 0, label = paste("Average:", weight_avg), hjust = -0.2, vjust = -1), color = "#c62d64") +
  geom_vline(
    xintercept = weight_median, 
    linetype = "dotted", 
    color = "#87ca9d",
    size = 1.5) +
   geom_text(aes(weight_median, 0, label = paste("Median:", weight_median), hjust = 1.2, vjust = -1), color = "#87ca9d") +
  ggtitle("Distribution of player's weight") +
  theme_report() +
  ylab("Weight (kg)") +
  xlab("Density")

```


#### **overall**

The average player's overall score is `r round(mean(data$overall),2)`, the median is equal to `r round(median(data$overall),2)`, the maximum is equal to `r round(max(data$overall),2)`, the minimum is equal to `r round(min(data$overall),2)`, and the standard deviation is `r round(sd(data$overall),2)`.

Below it is represented the histogram and the density of the variable overall. It is possible to see that the distribution of the overall score is symmetric around the average (`r round(mean(data$overall),2)`.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}

overall_avg <- round(mean(data$overall),2)
overall_median <- round(median(data$overall),2)

# histogram and density
data %>%
  ggplot(aes(x = overall)) +
  geom_histogram(
    aes(y = ..density..),
    bins = 15, 
    fill = "#509997", 
    color = "black") +
  xlim(c(45, 95)) +
  geom_density(color = "#761954", size = 1) +
  geom_vline(
    xintercept = overall_avg, 
    linetype = "dashed", 
    color = "#c62d64",
    size = 1.5) +
   geom_text(aes(overall_avg, 0, label = paste("Average:", overall_avg), hjust = 1.2, vjust = -1), color = "#c62d64") +
  geom_vline(
    xintercept = overall_median, 
    linetype = "dotted", 
    color = "#87ca9d",
    size = 1.5) +
   geom_text(aes(overall_median, 0, label = paste("Median:", overall_median), hjust = -0.2, vjust = -1), color = "#87ca9d") +
  ggtitle("Distribution of player's overall score") +
  theme_report() +
  ylab("Overall Score") +
  xlab("Density")

```

Lets also create a normal qqplot to see if the overall score fits a normal distribution. 

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=FALSE}
require(ggpubr)

# qqplot
data %>% 
  ggqqplot(
    x = "overall",
    color = "#761954",
    ggtheme = theme_report()) +
  ggtitle("Normal qqplot")

```

The normal qqplot compares the theoretical quantile from a normal distribution with the quantile from our sample data, in our case the overall variable. If the overall variable follows a normal distribution we expect to see most of the points above the reference line (Theoretical = Sample). Indeed, based on the normal qqplot, we can assume that the overall variable follows a normal distribution.


#### **preferred_foot**

There are `r nrow(data %>% filter(preferred_foot == 'Right'))` (`r round(nrow(data %>% filter(preferred_foot == 'Right'))/nrow(data)*100,2)`%) players that prefer to use the Right foot, and `r nrow(data %>% filter(preferred_foot == 'Left'))` (`r round(nrow(data %>% filter(preferred_foot == 'Left'))/nrow(data)*100,2)`%)players that prefer to use the Left foot, as you can see in the bar chart bellow. 


```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}

right_foot <- nrow(data %>% filter(preferred_foot == 'Right'))
left_foot <- nrow(data %>% filter(preferred_foot == 'Left'))

# bar plot
data %>%
  ggplot(aes(x = preferred_foot, fill = preferred_foot)) +
  geom_bar(color = "black") +
  ggtitle("Number of players by preferred foot") +
  ylab("Number of Players") +
  xlab("Preferred Foot") +
  stat_count(
    geom = "text",
    colour = "black", 
    size = 3.5,
    aes(label = ..count..), position=position_stack(vjust = 1.1)) +
  labs(fill = "Preferred Foot") +
  scale_fill_report()+
  theme_report()
 

```



#### **nationality**

For the *nationality* variable we are going to show only the first 5 countries with more players in the data set, as presented below:

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
# 5 nationalities with more players
top5_nat <- data %>%
  group_by(nationality) %>%
  summarise(
    n = n()) %>%
  ungroup() %>%
  mutate(
    prop = n/sum(n) *100
  ) %>%
  arrange(desc(n)) %>%
  top_n(5, n)

knitr::kable(top5_nat)
```

There are `r top5_nat %>% filter(nationality == 'England') %>% .$n` (`r round(top5_nat %>% filter(nationality == 'England') %>% .$prop, 2)`%) players from England, `r top5_nat %>% filter(nationality == 'Germany') %>% .$n` (`r round(top5_nat %>% filter(nationality == 'Germany') %>% .$prop, 2)`%) players from Germany, `r top5_nat %>% filter(nationality == 'Spain') %>% .$n` (`r round(top5_nat %>% filter(nationality == 'Spain') %>% .$prop, 2)`%) players from Spain, `r top5_nat %>% filter(nationality == 'France') %>% .$n` (`r round(top5_nat %>% filter(nationality == 'France') %>% .$prop, 2)`%) players from France, and `r top5_nat %>% filter(nationality == 'Argentina') %>% .$n` (`r round(top5_nat %>% filter(nationality == 'Argentina') %>% .$prop, 2)`%) players from Argentina as you can also see in the bar chart bellow. 

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}

# bar plot
top5_nat %>%
  ggplot(aes(x = nationality, y = n, fill = nationality)) +
  geom_col(color = "black") +
  ggtitle("Number of players by nationality") +
  ylab("Number of Players") +
  xlab("Nationality") +
  labs(fill = "Nationality") +
  scale_fill_report()+
  theme_report()
 

```


### Contingency Table

We are going to take our two categorical variables to create a contingency table and a mosaic plot, to verify the number using the left and the right foot for each one nationality of the top 5 nationalities presented above.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
# Contingency Table

data_cont <- data %>%
  filter(nationality %in% top5_nat$nationality)

cont_table <- table(data_cont$nationality, data_cont$preferred_foot)

knitr::kable(cont_table)
```

Also, below is presented the mosaic plot:

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
# mosaic plot
cont_table <- table(data_cont$nationality, data_cont$preferred_foot)

plot(cont_table)
```


## Comparing Groups

In this section we are going compare the overall score in terms of mean for the two classes of players: prefer to use the right foot vs prefer to use the left foot. To do that, first, we are going to to estimate the population mean for both subgroups (we will use the sample mean $\hat{\mu}$ estimator as presented in the previous chapter). 

The sample mean for the overall score variable was equal to `r mean(data %>% filter(preferred_foot == 'Right') %>% .$overal)` for the subset of  players which prefer to use the right foot, and equal to `r mean(data %>% filter(preferred_foot == 'Left') %>% .$overal)` for the subset of  players which prefer to use the left foot. Let's create a box-plot to make it more visual.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}

# Boxplot
data %>%
  ggplot(aes(x = preferred_foot, y = overall, fill = preferred_foot)) +
  geom_boxplot() +
  ggtitle("Box-Plot of player's overall score by preferred foot") +
  theme_report() +
  scale_fill_report() +
  ylab("Overall Score") +
  xlab("Prefered Foot")

```

It is possible to see that the players that preferred to use the left foot are a little above than the ones which preferred to use the right foot. Now the question is, is this difference significant? To discuss about it we are going run a t test using the R function `t.test()`:

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
#t test
test_result <- t.test(data$overall ~ data$preferred_foot)


```

The result presented above is testing the following hypothesis:

$$
H_0: \mu_1 = \mu_2 
$$
$$
H_1: \mu_1 \not= \mu_2
$$

So, the conclusion of this test is, at 5% significance, to reject the $H_0$ hypothesis, in other words, the players which prefer to use the left foot have significant better overall scores then the ones that prefer to use the right foot (p-value = `r test_result$p.value` < 0.05).



## Machine Learning

In this session we will test some algorithms to try to predict if the player prefer to use the left  foot based on other variables like weight, age, and overall. But first lest create a binary variable called "prefer_left".

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
# creating preferred_foot variable
dt_model <- data %>%
  mutate(
    prefer_left = if_else(preferred_foot == "Left", 1, 0)
  )

# selecting the a sample size 5
set.seed(1992)
row_sample <- sample(x = nrow(dt_model), size = 5, replace = FALSE)
data_sample_mod <- dt_model[row_sample,]

knitr::kable(data_sample_mod)
  
```

Now we can load the `caret` package and starting our models. The first step will be to divide our data set in two parts: training (80%) and test (20%). The training will be the part of the data using to train our models, while the test will be only used to test our predictions.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
require(caret)

# create Data Partition
intrain <- createDataPartition(dt_model$prefer_left, p = 0.8, list = FALSE)

set.seed(1992)

db_train <- dt_model[intrain,]
db_test <- dt_model[-intrain,]

```

The second step will be to define a "formula" for our models and also setting a 5-fold cross validation.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
# formula
form <- formula(prefer_left ~ age + weight_kg + overall)

# 5-fold cross validation
set.seed(1992)
tc <- caret::trainControl(method = "cv", number = 5)

```

Now we are going to train 2 models, a Logistic Regression and a Random Forest. Below is presented the result for the logistic regression.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
# logist Regression
set.seed(1992)
lr <- train(
  form,
  data = db_train, 
  method = "glm",
  trControl = tc)

summary(lr)

lr$finalModel
lr$results
```

Below is presented the results for the Random Forest model.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
# Random Forest 1
# tunning 500
set.seed(1992)
tg <- expand.grid(.mtry = seq(2, 10, by =2))

rf <- train(
  form,
  data = db_train, 
  method = "rf",
  trControl = tc,
  tuneGrid = tg,
  ntree = 500)

summary(rf)
rf$finalModel
rf$results
```

The last step will be to evaluate this two models in our test data set. Also, we will use as a cutoff 0.3, in other words scores bigger than 0.5 will be 1 (prefer to use the left foot) otherwise 0. 

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}

db_test_score <- db_test %>%
  mutate(
    predict_lr = predict(lr, db_test),
    predict_rf = predict(rf, db_test),
    predict_lr_cat = if_else(predict_lr > 0.3, 1, 0),
    predict_rf_cat = if_else(predict_rf > 0.3, 1, 0)
  )


```

Below is presented the confusion matrix of our prediction using the logistic regression model applied in the test data set.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
# confusion matrix logistic
ModelMetrics::confusionMatrix(actual = db_test_score$prefer_left, predicted = db_test_score$predict_lr, cutoff = 0.3)

```

Below is presented the confusion matrix of our prediction using the random forest model applied in the test data set.

```{r, eval=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE}
# confusion matrix random forest
ModelMetrics::confusionMatrix(actual = db_test_score$prefer_left, predicted = db_test_score$predict_rf, cutoff = 0.3)


```

It is possible to see that the both models were not that good to predict if the player prefer to use the left foot, maybe we could include more variables or try to create categories for the continuous predictors. However the random forest was better to classify the "1" than the logistic regression.



